\section{Experiments}
%\subsection{Experimental Setup}
%\textbf{Dataset and Evaluation Methodology}.
Our experiments are designed to answer the following questions,
1) whether it is possible to automatically collect training data for event extraction,
2) whether  extractors trained on such data can detect events of interest and identify
their corresponding arguments,
and 3) whether our solution can work with other knowledge resources for more types of event.

 \subsection{Dataset Evaluation}\label{sec:evalhypo}
Let us first evaluate \textbf{different data-collecting strategies discussed in Section XXXX}:
(1) \emph{ALL}: all arguments are regarded as key arguments; (2) \emph{IMP}: we select the top half arguments with highest importance values as key arguments; (3) \emph{IMP\&TIME} means adding a time-related argument with the highest importance value to %the set of key arguments defined by
\emph{IMP}; (4) \emph{DIS}: we eliminate sentences where the dependency distances between any two key arguments are greater than 2.
We follow the above methods to collect datasets using Freebase and the English Wikipedia dump of 2016-11-20,
randomly select 100 sentences from each dataset, and ask two annotators to decide whether each sentence implies a given type of event.



%To investigate the possibility of automatically constructing training data for event extraction,

\begin{table}[h]
\small
\centering
\begin{tabular}{|c|l|c|c|c|} \hline
	No. & Strategy & Sent. & Type & Post. \\ \hline
	T1 & \emph{ALL} & 203 & 9 & 98\% \\ \hline
	T2 & \emph{IMP} & 318K & 24 & 22\% \\ \hline
	T3 & \emph{IMP}+\emph{DIS} & 170K & 24 & 37\% \\ \hline
	T4 & \emph{IMP\&TIME} & 112K & 24 & 83\% \\ \hline
	T5 & \emph{IMP\&TIME}+\emph{DIS} & 46K & 24 & 91\% \\ \hline
\end{tabular}
\caption{Statistics of the datasets built with different strategies.
% \textit{Instances} denotes the number of CVT instances that can be used for each hypothesis.
\textit{Sent.} is the number of sentences found. \textit{Type} the number of different CVT types found.  \textit{Post.} is the percentage of sentences mentioning the given events explicitly.\label{tab:3}}
\end{table}

As shown in Table~\ref{tab:3}, it is not surprising that \emph{T1}, as the most strict, guarantees the quality of the collected data, but only contributes 203 sentences covering 9 event types, which is far from sufficient for further applications. \emph{T2} relaxes \emph{T1} by allowing the absence of non-key arguments, which expands the resulting dataset, but introduces more noise.
We can also see that the dependency constraint (DIS) improves the data quality (\emph{T3}).
%indicating that \emph{T2} is inappropriate to be used as a soft constraint.
Compared with \emph{T2}, the significant quality improvement by \emph{T4} proves that time-related arguments within CVT schemas are critical to imply an event occurrence. Among all strategies, the dataset by \emph{T5} achieves the best quality, while still accounting for 46735 sentences with 50109 events, almost 8 times more than the ACE dataset, showing that it is feasible to automatically collect quality training data for event extraction without either human-designed event schemas or \textbf{extra} human annotations.
% our hypothesis \emph{H3} and \emph{H4} are feasible and it is an effective way to generate reliable data automatically.

Our final dataset, \textbf{FBWiki}, using \emph{T5}, contains 46,735 positive sentences and 79,536 negative ones\footnote{To train a better
system, besides trivial negative samples that have no arguments, we then randomly sample 34,837 negative instances that contain serveral
but not all key arguments, and 21,866 sentences whose key arguments violate the dependency constraint.}
 a random split of 101,019 for
training and 25,252 for testing. Note that there are averagely 4.8 arguments per event, and in total, 5.5\% instances labeled with more
than two types of events.
%Statistics for the generated dataset from Freebase is shown in Table~\ref{statistics}.

%\begin{table}
%\small
%\centering
%\begin{tabular}{|l|c|c|c|} \hline
%& Train & Dev & Test \\ \hline
%\emph{\#PosSent.} & 29912 & 7477 & 9346 \\ \hline
%\emph{\#NegSent.} & 50904 & 12726 & 15906  \\ \hline
%\emph{\#Eve.} &  &  &  \\ \hline
%\emph{\#Arg.} &  &  &  \\ \hline
%\emph{\%Multi\_Eve.} &  &  &  \\ \hline
%\end{tabular}
%\caption{Statistics for the generated dataset. \emph{\#PosSent.} is the number of positive sentences, \emph{\#NegSent.} is the number of positive sentences, \emph{\#Eve.} is the number of event mentions, and \emph{\#Arg.} is the number of event arguments. \emph{\%Multi\_Eve.} is the ratio of multi-type events.
%, and \emph{\%Multi\_Arg.} is the ratio of multi-word arguments.
%\label{statistics}}
%\end{table}

%%%% containing 7,180 sentences, containing 7,394 events and 25,840 arguments. We then randomly select 4,800 sentences for training and 1,180 sentences as test set, and the rest 1,200 sentences for validation.
%We first manually evaluate the quality of our test set and then regard the automatically generated data as gold standard and evaluate our model accordingly. Next, we manually evaluate a subset of events detected by our model and analyze the differences with regards to the automatic evaluation. Finally, we conduct evaluation on a smaller dataset constructed according to Wikipedia tables and articles.

\paragraph{Trigger Inference:} To further explore the relations between key arguments and triggers, we regard the least common ancestor of all key arguments in the dependency parse tree as a trigger candidate. As listed in Table~\ref{freqTriggers}, these trigger candidates share similar lexical meanings and are highly informative to infer their corresponding event types. Though missing explicit information from \KB, most triggers can be inferred from the dependencies between a group of key arguments and its context.

\begin{table}
	\small
	\centering
	\begin{tabular}{|l|l|l|} \hline
		Event types & Trigger candidates & Prop. \\ \hline
		film\_performance & play, appear, star, cast, portray & 0.72 \\ \hline
		award\_honor & win, receive, award, share, earn & 0.91 \\ \hline
		education & graduate, receive, attend, obtain, study & 0.83 \\ \hline
		acquisition & acquire, purchase, buy, merge, sell & 0.81 \\ \hline
		employment\_tenure & be, join, become, serve, appoint & 0.79 \\ \hline
	\end{tabular}
	\caption{Top 5 most frequent trigger candidates and the proportion of their appearance over all postive instances of five event types.}
	\label{freqTriggers}
\end{table}

\paragraph{On ACE:} We also test our strategy on the ACE dataset. 
We first collect all annotated events, without triggers, as the knowledge base
to compute the importance values for all arguments, and select the key arguments
for each ACE event type accordingly.  We follow \textit{T5} to examine every sentence
whether it can be selected as an annotated instance within the ACE event types.
Eventually, we correctly obtain 3448 sentences as positive instances, covering 
64.7\% of the original ACE dataset.  We find that the main reason for the 
missing 35.3\% is that many arguments in the ACE dataset are pronouns, 
where our strategy is currently unable to treat a pronoun as an key argument. However,
if a high-precision coreference resolution tool is available to preprocess the document,
our solution would be able to automatically label more instances as training data.
% \textbf{ANYTHING NOVEL that traditional ACE does not have??? multple types? multiple events?}

 \subsection{Extraction Setup}\label{sec:evalevent}
Next, we evaluate our event extractor on the collected datasets,
%\paragraph{Metrics:} We evaluate our models
in terms of popular metrics in event extraction, i.e., precision (P), recall (R), and F-measure (F) for each subtask. These metrics are computed according to the following standards of correctness:
For \emph{event classification}, an event is correctly classified if its reference sentence contains \textbf{all key arguments} of this event type.
For \emph{key argument detection}, an event is correctly detected if its type and all its key arguments match a reference event within the same sentence.
For \emph{all argument detection}, an event is correctly extracted if its type and all its arguments match a reference event within the same sentence.

\paragraph{Training:} All hyper-parameters are tuned on a development split in the training set. During event detection, we set the size of word embeddings to 200, the size of LSTM layer to 100. In argument detection, we use the same size of word embedding, while the size of LSTM layer is 150, and the size of key argument embedding is 50. Word embeddings are pre-trained using skip-gram word2vec~\cite{mikolov2013distributed} on English Wikipedia and fine tuned during training. We apply dropout (0.5) on both input and output layers.




\paragraph{Baselines:}
%To investigate the effectiveness of our proposed model,
We compare our proposed model with three baselines. %including traditional feature-based and neural network models. All baselines are trained
%following a \textbf{two-step pipeline, i.e., event detection and argument detection.?????}
%For neural network baslines, we train
The first is a  BLSTM model that takes word embeddings as input, and outputs the label for each word with the maximum probability. % among all possible labels.
For feature-based methods, we apply Conditional Random Field \cite{lafferty2001conditional} (with the CRF++ toolkit~\cite{kudo2005crf++} ) and Maximum Entropy \cite{berger1996maximum} (Le Zhang's MaxEnt toolkit) to explore a variety of elaborate features, such as lexical, syntactic and entity-related features, according to the state-of-art feature-based ACE event extraction system~\cite{li2013joint}.
% Note that during argument detection stage, we add the event detection label for each word as a supplementary feature.

All models are first required to predict each word with a label indicating whether \textbf{it is in an key argument, and add the resulting label of each word as a supplementary feature to predict whether it is in a non-key argument}.

%We use Stanford CoreNLP \cite{manning2014stanford} for feature extraction, and utilize the CRF++ toolkit~\cite{kudo2005crf++} and Le Zhang's MaxEnt toolkit \footnote{\url{https://github.com/lzhang10/maxent}} to train the CRF and Max Entropy classifiers, respectively.

\subsection{Compare with Automatic Annotations}
Firstly, we compare the model output against with the automatically obtained event annotations.
As shown in Table~\ref{tab:1}, traditional feature-based models perform worst in both event detection and argument detection.
One of the main reasons is the absence of explicit event trigger annotations in our dataset, which makes it impossible to include trigger-related features, e.g., trigger-related dependency features and position features.
Although traditional models can achieve higher precisions, they only extract a limited number of events, resulting in low recalls.
Neural-network methods perform much better than feature-based models, especially in recall, since they can make better use of word semantic features. Specifically, BLSTM can capture longer dependencies and richer contextual information, instead of neighbouring word features only.
And the CRF component brings an averagely 2\% improvement in all metrics, and by adding the ILP-based post inference module, our full model, BLSTM-CRF-ILP$_{multi}$, achieves the best performance among all models.
% Moreover, neural-network-based methods can avoid errors propagating from other NLP preprocessing tools like POS tagging and NER.

\begin{table*}[!t]
\centering
\small
\begin{tabular}{|l|p{1.cm}<{\centering}|p{1.cm}<{\centering}|p{1.cm}<{\centering}|p{1.cm}<{\centering}|p{1.cm}<{\centering}|p{1.cm}<{\centering}|p{1.cm}<{\centering}|p{1.cm}<{\centering}|p{1.cm}<{\centering}|} \hline
	\multirow{2}{*}{Model} & \multicolumn{3}{c|}{Event Classification} & \multicolumn{3}{c|}{Key Argument Detection} &
	\multicolumn{3}{c|}{All Argument Detection} \\ \cline{2-10}
	 & P & R & F & P & R & F & P & R & F \\ \hline
	CRF & 88.9 & 11.0 & 19.6 & 36.1 & 4.47 & 7.96 & 19.9 & 3.06 & 5.30  \\ \hline
	MaxEnt & \textbf{95.2} & 12.4 & 21.9 & 41.6 & 5.40 & 9.56 & 22.5 & 3.40 & 5.91 \\ \hline
	BLSTM & 89.8 & 63.0 & 74.1 & \textbf{64.9} & 45.5 & 53.5 & 42.9 & 27.7 & 33.7  \\ \hline \hline
	BLSTM-CRF & 86.4 & 67.4 & 75.7 & 63.6 & 49.6 & 55.8 & \textbf{44.4} & 31.0 & 36.5  \\ \hline
	BLSTM-CRF-ILP$_{1}$ & 84.4 & 74.1 & 78.9 & 62.3 & 53.8 & 57.3 & 42.7 & 33.8 & 37.7 \\ \hline
	BLSTM-CRF-ILP$_{multi}$ & 85.3 & \textbf{79.9} & \textbf{82.5} & 60.4 & \textbf{55.3} & \textbf{57.7} & 41.9 & \textbf{34.6} & \textbf{37.9} \\ \hline
\end{tabular}
\caption{System performance when compared with automatic annotations (\%).  \label{tab:1}}
\end{table*}

% \paragraph{Multi-word Argument Detection}
% Committing to the multi-word argument issue, we treat each subtask as a sequence labeling problem. Evaluated on multi-word arguments, the F1 scores of BLSTM-CRF, BLSTM-CRF-ILP$_1$ and BLSTM-CRF-ILP$_{multi}$ in argument detection are 71.3\%, 80.5\%, and 81.0\%, respectively.

\paragraph{The CRF Layer:}
%As seen in Table~\ref{tab:1},
It is not surprising that  every model with a CRF layer over its BLSTM layer is superior to the one with a BLSTM layer only. Compared with vanilla BLSTM, BLSTM-CRF achieves higher precisions and recalls in all subtasks by significantly reducing the invalid labelling sequences (e.g., \texttt{I-arg} appears right after \texttt{O}). During prediction, instead of tagging each token independently, BLSTM-CRF takes into account the constraints between neighbouring labels, and potentially increases the co-occurrences of key arguments regarding the same event type. % in some way.

\paragraph{The ILP Post Inference:}
As shown in Table~\ref{tab:1}, ILP-based post inference considerably improves the overall system performance, especially in \textit{event type classification}. With the help of constraint \textbf{C4},  dubious key arguments can be correctly inferred through other key arguments from their context. Compared with BLSTM-CRF, BLSTM-CRF-ILP$_1$ produces an F1 gain of 3.2\% in event classification, 1.5\% in key argument detection, and 1.2\% in all argument detection. %, with respect to tF1.

\paragraph{Multi-type Events:}
%We further investigate the effect of
Among all methods, BLSTM-CRF-ILP$_{multi}$ is the only model that can deal with multi-type event mentions. %As shown in Table~\ref{tab:1},
The proposed strategy in BLSTM-CRF-ILP$_{multi}$ helps detect more event mentions for a sentence, contributing to the increase of recalls, and F1 scores with a little drop of precisions.
%\textbf{Evaluated on \textbf{38XXXX} sentences containing multi-type event mentions, the F1 scores of BLSTM-CRF-ILP$_{multi}$ in event type classification, event detection and argument detection are 70.7\%, 58.4\% and 26.9\%, respectively.}
BLSTM-CRF-ILP$_{multi}$ can correctly identify 132 sentences with multi-type events,
with an accuracy of 95.6\%, and for each involved event, our model maintains a high
performance in identifying its corresponding arguments, achieving the F1 scores of 45.5\%, and 29.1\% for key argument detection and all argument detection, respectively.

\subsection{Manual Evaluation}\label{manualeve}
To provide a deep investigation about our dataset and models, we randomly sample 150 sentences
from the test set. Two annotators are asked to annotate all arguments to each sentence following
two steps. First, determine whether a given sentence is positive or negative, and assign \textbf{all existing}
event types to positive ones. Next, label all related arguments and their roles according to
the event types for all positive instances. Two annotators will independently annotate each sentence,
and discuss to reach an agreement. The inter-annotator agreement is 87\% for event types and 79\% for arguments.


By comparing the automatic and manual annotations on the 150 sentences, we find that
the main issue for the automatic annotation is that some automatically labeled sentences
do not imply any event while still matching all key properties of certain CVT entries in Freebase.
We find 16 such instances that are mistakenly labeled as positive.
For example in Figure~\ref{fig:1}, although the phrase \underline{\emph{the car}} in S6 matches a film name,
it does not indicate that film. %, and there is no explicit evidence indicating that an actor starred in this film.
This is mainly because that we currently do not have a strong entity linker to verify those entities, which we leave for
future work.
But, interestingly,
% a bottleneck of our data generation strategy.
during manual investigation,
%we find 16 negative sentences which are mistakenly labeled as positive, and
we find that BLSTM-CRF-ILP$_{multi}$ manages to detect 6 of them as negative.


\begin{table}[h]
\small
\centering
\begin{tabular}{|l|p{0.8cm}<{\centering}|p{0.8cm}<{\centering}|p{0.8cm}<{\centering}|} \hline
	% Model & EC & AD & ED \\ \hline
	% CRF & 21.2 & 13.3 & 5.30 \\ \hline
	% MaxEnt & 17.7 & 11.7 & 5.44 \\ \hline
	% BLSTM & 79.8 & 64.3 & 41.2 \\ \hline \hline
	Model & EC & KAD & AAD \\ \hline
	BLSTM-CRF & 75.4 & 54.7 & 35.5 \\ \hline
	BLSTM-CRF-ILP$_{1}$ & 77.2 & 56.1 & 35.7 \\ \hline
	BLSTM-CRF-ILP$_{multi}$ & \textbf{80.7} & \textbf{56.4} & \textbf{36.3} \\ \hline
\end{tabular}
\caption{The F1 scores of different systems on the manually annotated data. EC, KAD, AAD denote the event type classification, key argument detection and all key argument detection, respectively. \label{tab:2}}
\end{table}

Table~\ref{tab:2} summarizes the performances of different models on the manually annotated dataset, where we can observe similar trends with Table~\ref{tab:1}, that
both CRF and ILP$_{multi}$ greatly improve the overall performance and
% shows the system performances in the manual evaluation. We can draw similar conclusions about the comparison of performances between different models as automatic evaluation. It is clear that
BLSTM-CRF-ILP$_{multi}$ remains the most effective model. %,  in event extraction as it achieves the highest F1 scores in both manual and automatic evaluation.

\begin{figure}[h]
	\centering
	\includegraphics[width=.48\textwidth]{figure3.png}
	\caption{Example outputs of BLSTM-CRF-ILP$_{multi}$.\label{fig:1}}
\end{figure}



%Moreover, %manual evaluation may help us to gain a deep insight about our data and models. We
% we manually check the top 5 event types whose EC F1 scores by BLSTM-CRF-ILP$_{multi}$ differ greatly between automatic evaluation and manual evaluation, summarized in Table~\ref{tab:4}. We find that most of the performance differences are caused by data generation. Figure~\ref{fig:1} examples two types of errors in data construction. Several automatically labeled sentences do not imply any event while still matching all key properties of some CVT entries. For example, although the phrase \emph{the car} in S5 matches a film name, it does not indicate this film, and there is no explicit evidence indicating that an actor starred in this film. This is a bottleneck of our data generation strategy. During manual evaluation, we find 16 negative sentences which are mistakenly labeled as positive, and our model manages to rectify 6 of them.

Remarkably, our BLSTM-CRF-ILP$_{multi}$ model can find more CVT instances that are currently not referenced in Freebase. Our model detects two events in S7, while the arguments of the second event do not match any existing CVT instances in Freebase, which do not receive any credit during automatic evaluation, but should be populated into Freebase. %leading to a missing event in data generation.
This phenomenon suggests that by learning from distant supervision provided by Freebase, our model can be used to populate or update Freebase instances in return.

%\begin{table}[h]
%\small
%\centering
%\begin{tabular}{|l|c|c|c|} \hline
%	Event type & P & R & F \\ \hline
%	olympics.medal\_honor%\footnote{The full name is olympics.olympic\_medal\_honor in Freebase.}
%	& $\downarrow$ 25.0\% & $\downarrow$ 5.0\% & $\downarrow$ 13.8\% \\ \hline
%	film.performance & $\downarrow$ 21.4\% & $\uparrow$ 3.1\% & $\downarrow$10.3\% \\ \hline
%	business.acquisition & $\rightarrow$ & $\downarrow$ 7.1\% & $\downarrow$ 5.4\% \\ \hline
%	tv.appearance%\footnote{The full name is tv.regular\_tv\_appearance in Freebase.}
%	& $\downarrow$ 9.5\% & $\uparrow$ 3.0\% & $\downarrow$ 3.1\% \\ \hline
%	film.release%\footnote{The full name is film.film\_regional\_release\_date in Freebase.}
%	& $\downarrow$ 7.7\% & $\uparrow$ 5.6\% & $\downarrow$ 0.55\% \\ \hline
%\end{tabular}
%\caption{The difference of EC F1 scores (by BLSTM-CRF-ILP$_{multi}$) between automatic and manual evaluation for top 5 event types.\label{tab:4}}
%\end{table}

\paragraph{On BBC News:}
% 在BBC新闻上抽取business.acquisition和tv.regular_tv_appearance两类事件，并用规则更新
We further apply our event extractor, trained on the FBWiki, to 397 BBC News
articles (from 2017/04/18 to 2017/05/18 in Politics, Business and TV sections), and manually examine the extraction results. We find that our model is able to identify 117 events, and 53 events,
almost half of which are not covered in the currently used Freebase.



\subsection{Tables as Indirect Supervision}
To investigate the applicability of our approach to other structured knowledge/tables besides Freebase CVT tables, %we conduct manual evaluation on
we automatically build a new dataset, TBWiki, with the supervision provided by Wikipedia tables, which characterize events about business acquisition, winning of the Olympics games, and winning prestigious awards in entertainment (Table~\ref{tab:6}).

\begin{table}[h]
\small
\centering
\begin{tabular}{|l|c|c|c|c|c|} \hline
	Event type & Entr. & Sent. & EC & KAD & AAD \\ \hline
	Acquisition & 690 & 414 & 87.0 & 72.0 & 69.6 \\ \hline
	Olympics & 2503 & 1460 & 77.2 & 64.5 & 38.6 \\ \hline
	Awards & 3039 & 2217 & 95.0 & 82.8 & 58.6 \\ \hline
\end{tabular}
\caption{Statistics of the TBWiki dataset and the performance of our model on TBWiki. \textit{Entr.} is the number of table entries. \textit{Sent.} is number of positive instances.\label{tab:6}}
\end{table}

We train our BLSTM-CRF-ILP$_{multi}$ on this dataset and evaluate it on 100 manually annotated sentences.
% and follow the same steps of event annotations as mentioned in Section~\ref{manualeve}.
We can see that without extra human annotations, %Table~\ref{tab:6} demonstrates that tabular data as distant supervision can be adapted to extract high-confidence events
our model can learn to extract events from the training data weakly supervised by Wikipedia tables. Given a specific event type, as long as we can acquire tables implying events of such type, it is possible to automatically collect training data from such tables, and learn to extract structured event representations of that type. % an effective but robust event extractor. , which is much easier than human annotation and unlimited in event types.
