\section{Training Data Generation}
Our approach exploits structural information like \FB \CVT tables to  automatically annotate event mentions\footnote{An event mention is a
phrase or sentence within which an event is described, including its type and arguments.}, and generate training data to learn an
event extractor. We use the arguments of a \CVT table entry to infer what event a sentence is likely to express. A \CVT table entry can
have multiple arguments, but not all of them are useful in determining the occurrence of an event. For instance, the
\texttt{divisions\_formed} argument in Figure~\ref{fig:example}(b) is not as important as the other three arguments when determining if a
sentence expresses a \texttt{business.acquisition} event.


Our first step for training data generation is to identify the key arguments from a \CVT table entry. A \textbf{key argument} is an
argument that plays an important role in one event, which helps to distinguish with other events. If a sentence contains all key arguments
of an entry in an event table (e.g. a \CVT table), it is likely to express the event presented by the table entry. If a sentence is
labelled as an event mention of a \CVT event, we also record the words or phrases that match the entry's properties as the involved
arguments, with the roles specified by their corresponding property names.

%For instance, sentence S1 shown in Figure~\ref{fig:example} (a)
%is a mention of \texttt{business.acquisition} type, its involved arguments are ``Remedy Corp", ``BMC Software", and ``2004", and the roles
%of the three arguments are \texttt{company\_acquired}, \texttt{acquiring\_company} and \texttt{date} respectively.

\subsection{Determining Key Arguments}
We use the following formula to calculate the importance score, $I_{cvt, arg}$, of an argument \emph{arg} (e.g., date) to its event type
\emph{cvt} (e.g., \texttt{business.acquisition}):

\begin{equation}
	I_{cvt, arg} = log \frac{count(cvt, arg)}{count(cvt) \times count(arg)}
\end{equation}
%
where $count(cvt)$ is the number of instances of type $cvt$ within a \CVT table, $count(arg)$ is the number of times $arg$ appearing in all
\CVT types within a \CVT table, and $count(cvt, arg)$ is the number of $cvt$ instances that contain $arg$ across all \CVT tables.
\FIXME{explain how do you come up with this formula.}


Our strategy for selecting key arguments of a given event type is described as follows:
%
\begin{description}

\item [P1] For a \CVT table with $n$ arguments, we first calculate the importance score of each argument. We then consider the top half
    $\ceil[\big]{n/2}$ (rounding up) arguments that have the highest importance scores as key arguments.

\item [P2] We find that time-related arguments are useful in determining the event type, so we always include a time-related argument\footnote{If there are multiple time-related arguments, we select the one with highest importance score.}
    (such as date) in the key argument set.

\item [P3] We also remove sentences from the generated dataset in which the dependency distances between any two key arguments are
    greater than 2. The distance between two arguments is the minimal number of hops it takes from one argument to the other on the dependency parse tree (see also Figure~\ref{fig:2}).

\end{description}
%
Using this strategy, the first three arguments of the \CVT entry are considered to be key arguments for event type
\texttt{business.acquisition}.


\paragraph{Selection Creteria: }
To determine how many arguments should be chosen in P1, we have conducted a series of evaluations on the quantity and
quality of the datasets using different policies. We found that choosing the top half arguments % (sorted by their importance scores)
gives the best accuracy for event labeling.

We use the following three example sentences from Wikipedia% text dataset
to explain P2 and P3 %in our key argument selection strategy
described above. %The three sentences are:
%
\begin{quote}
\textbf{S2}: \underline{\emph{Microsoft}} spent \$6.3 billion buying online display advertising company \underline{\emph{aQuantive}} in
\underline{\emph{2007}}.
\end{quote}
\begin{quote}
\textbf{S3}: Microsoft hopes aQuantive's Brian McAndrews can outfox Google.
\end{quote}
\begin{quote}
\textbf{S4}: On April 29th, Elizabeth II and Prince Philip witnessed the marriage of Prince William.
\end{quote}

\begin{table}
 \scriptsize
        \begin{tabular}{llllc}
        \toprule
        id & company\_acquired & acquiring\_company & date & divisions\_formed\\
        \midrule
        m.05nb3y7 & aQuantive & Microsoft & 2007 & N/A\\
        \bottomrule
        \end{tabular}
        \vspace{-2mm}
        \caption{A \texttt{business.acquisition} \CVT entry in \FB. \label{tbl:bs}}
        \vspace{-2mm}
\end{table}


\begin{figure}
\centering
	\includegraphics[width=.48\textwidth]{figure2.png}
    \vspace{-5mm}
	\caption{The dependency tree of S4, which partially matches a \CVT entry of \emph{people.marriage} in \FB. \label{fig:2}}
    \vspace{-2mm}
\end{figure}


Although time-related arguments are often missing in the currently imperfect \KBs, they are crucial to identify the actual occurrence of an
event. As an example, suppose we want to use the \CVT entry shown in Table~\ref{tbl:bs} to determine whether sentences S2 and S3 are
\texttt{business.acquistion} events. While this strategy works for S2, it gives a false prediction for S3. If we add the time-related
argument (i.e., date: 2007), we can then correctly label both sentences.
%We have conducted pilot experiments on our training data during the design phase, and found time-related arguments to be useful.
Therefore, we always consider the most important time-related argument as key
arguments (P2) when they are available.

Finally, P3 is based on our intuitions that two arguments involved in the same event mention are likely to be closer within the
syntactic structure. %As an example, consider
%Figure~\ref{fig:2} shows the dependency parse tree of S4, its  is given in .
%For example,  %in the dependency parse tree of S4 in Figure~\ref{fig:2},
As shown in  Figure~\ref{fig:2}, although both \emph{Prince Philip} and \emph{marriage} can be matched as key arguments in a  \texttt{people.marriage}
entry, but with a distance of 3 (i.e., far from each other under our criterion) on the dependency tree, thus S4 will be labeled as
negative.

\subsection{Data Generation}
To generate training data, we follow a number of steps.


Our approach takes in existing structured tables or lists that are organized in a way similar to the \FB \CVT tables. The structured tables
can be obtained from an existing knowledge base, created by experts, or generated through a combination of both approaches. For each event
type, we determine the key arguments for each entry within that type used the key argument selection strategy described above. This step
produces a set of rules to be used for data labeling, where each rule contains the event type, key arguments and non-key arguments given by
a structured table entry. We also use alias information (such as Wikipedia redirect) to match two arguments that have different literal
names but refer to the same entity (e.g. Microsoft and MS).

Next, we label each individual sentence from the target dataset. The labeling process is straightforward. We enumerate all rules from the
generate rule set, and check if the target sentence contains all the key arguments specified by a rule. We regard a sentence as a
\emph{positive} sample if it contains all the key arguments of a rule, or \emph{negative} otherwise. For instances, S1 in Figure~\ref
{fig:example}(a) and S2 (with its arguments in italics and underlined) are positive examples, while S3 and S4 are negative.

Because 68\%  of  arguments in our dataset consist of more than one word, we formulate the training in a sequence labeling paradigm rather
than word-level classifications. We tag each word of the sentence using the standard begin-inside-outside (\texttt{BIO})
scheme, where each token is labeled as \texttt{B-role} if it is the beginning of an event argument with its %corresponding
role \texttt{role}, or \texttt{I-role} if it is inside a \texttt{role}, or \texttt{O} otherwise. We call this a \textbf{labeling sequence}.
%For instance, Figure~\ref{fig:ls} illustrates the labeling sequence for sentence S1 given in Figure~\ref{fig:example} (a).  As an output, we
%record the raw text of each training sentence, plus its labeling sequence, and key/non-key arguments.

%\begin{figure}[t!]
%\centering
%\scriptsize
%\begin{tabular}{ccccccc}
%\toprule
%Remedy & Corp & was & sold & to & BMC & \\
%\rowcolor{Gray} \texttt{B-com.\_ac.} & \texttt{I-comp.\_ac.} & \texttt{O} & \texttt{O} & \texttt{O} & \texttt{B-ac.\_comp.} &\\
%\end{tabular}
%\begin{tabular}{ccccc}
%Software & as & the & Service &Management \\
%\rowcolor{Gray} \texttt{I-acq.\_comp.} & \texttt{O} & \texttt{O} & \texttt{B-div.\_fo.} & \texttt{I-div.\_fo.} \\
%Business & Unit & in & 2004 &.\\
%\rowcolor{Gray} \texttt{I-div.\_fo.} & \texttt{I-div.\_fo.} & \texttt{O} & \texttt{B-date} &\\
%\bottomrule
%\end{tabular}
%
%\caption{The labeling sequence for sentence S1 shown in Figure~\ref{fig:example} (a). Each token of the sentence is tagged using the \BIO
%scheme. \label{fig:ls}}
%\end{figure}

\subsection{Limitations}
Our approach relies on structured tables or lists to automatically label text. The table can be obtained from an existing \KB or
hand-crafted by experts. We stress that providing a table incurs much less overhead than manually tagging each training sentence, as a
single table entry can be automatically applied to an unbounded number of sentences.

%Like all current event extractors, our approach does not support detecting event types that are not seen in the training data. However, our
%approach does offer an easy way to extend the structured table to support new types, i.e., by adding new entries to the table.


%Our current implementation does not disambiguate pronouns. There are co-reference methods\FIXME{~\cite{}} that can be used to find out
%which word a pronoun refers to. These works are therefore complementary to our approach. Integrating our work with co-reference methods is
Our implementation may benefit from pronoun resolution, % disambiguate pronouns. %There are co-reference methods\FIXME{~\cite{}} that can be used to find out which word a pronoun refers to.
%These works are therefore complementary to our approach. Integrating our work with co-reference methods is our future work.
which is complementary to our method, and may improve our recall.
%Finally,
While this work targets the sentence level, we believe it is generally applicable. There are methods like \FIXME{\cite{}} can be used to
extend our approach to document-level event extraction.
