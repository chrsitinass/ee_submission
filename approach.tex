\section{Training Data Generation}
Our approach exploits structural information like \FB \CVT tables to  automatically annotate event mentions\footnote{An event mention is a
phrase or sentence within which an event is described, including its type and arguments.} in order to generate training data to learn an
event extractor. We use the arguments of a \CVT table entry to infer what event a sentence is likely to express. A \CVT table entry can
have multiple arguments but not all of them are useful in determining the occurrence of an event. For instance, the
\texttt{divisions\_formed} argument in Figure~\ref{fig:example} (b) is not as important as the other three arguments when determining if a
sentence expresses a \texttt{business.acquisition} event.


Our first step for training data generation is to identify the key arguments from a \CVT table entry. A \textbf{key argument} is an
argument that plays an important role in one event, which helps to distinguish with other events. If a sentence contains all key arguments
of an entry in an event table (e.g. a \CVT table), it is likely to express the event expressed by the table entry. If a sentence is
labelled as an event mention of a \CVT event, we also record the words or phrases that match the entry's properties as the involved
arguments, with the roles specified by their corresponding property names.

%For instance, sentence S1 shown in Figure~\ref{fig:example} (a)
%is a mention of \texttt{business.acquisition} type, its involved arguments are ``Remedy Corp", ``BMC Software", and ``2004", and the roles
%of the three arguments are \texttt{company\_acquired}, \texttt{acquiring\_company} and \texttt{date} respectively.

\subsection{Determining Key Arguments}
We use the following formula to calculate the importance score, $I_{cvt, arg}$, of an argument \emph{arg} (e.g., date) to its event type
\emph{cvt} (e.g., \texttt{business.acquisition}):

\begin{equation}
	I_{cvt, arg} = log \frac{count(cvt, arg)}{count(cvt) \times count(arg)}
\end{equation}


where $count(cvt)$ is the number of instances of type $cvt$ within a \CVT table, $count(arg)$ is the number of times $arg$ appearing in all
\CVT types within a \CVT table, and $count(cvt, arg)$ is the number of $cvt$ instances that contain $arg$ across all \CVT tables.
\FIXME{explain how do you come up with this formula.}


Our strategy for selecting key arguments of a given event type is described as follows:

\begin{description}

\item [P1] For a \CVT table with $n$ arguments, we first calculate the important score of each argument. We then consider the top half
    $\ceil[\big]{n/2}$ (rounding up) arguments that have the highest importance scores as key arguments.

\item [P2] We find that time-related arguments are useful in determining the event type, so we always include time-related arguments
    (such as date) in the key argument set.

\item [P3] We also remove sentences from the generated dataset in which the dependency distances between any two key arguments are
    greater than 2. The distance is the minimal number of hops it takes from one argument to the other on the directed dependency tree
    (see also Figure~\ref{fig:2}).

\end{description}

Using this strategy, the first three arguments of the \CVT entry are considered to be key arguments for event type
\texttt{business.acquisition}.


\subsection{Key Argument Selection Parameters}
To determine how many arguments should be chosen (step P1 in our strategy), we have conducted a series of evaluations on the quantity and
quality of the datasets using different policies. We found that choosing the top half arguments (sorted by their importance scores) gives
the best accuracy for event labeling.

We use three example sentences from the Wiki text dataset to explain steps P2 and P3 in our key argument selection strategy described
above. The three sentences are:

\begin{quote}
\textbf{S2}: \underline{\emph{Microsoft}} spent \$6.3 billion buying online display advertising company \underline{\emph{aQuantive}} in
\underline{\emph{2007}}.
\end{quote}
\begin{quote}
\textbf{S3}: Microsoft hopes aQuantive's Brian McAndrews can outfox Google.
\end{quote}
\begin{quote}
\textbf{S4}: On April 29th, Elizabeth II and Prince Philip witnessed the marriage of Prince William.
\end{quote}

\begin{table}
 \scriptsize
 \caption{\CVT entry of \texttt{business.acquisition} in \FB. \label{tbl:bs}}
        \begin{tabular}{llllc}
        \toprule
        id & company\_acquired & acquiring\_company & date & divisions\_formed\\
        \midrule
        m.05nb3y7 & aQuantive & Microsoft & 2007 & N/A\\
        \bottomrule
        \end{tabular}
\end{table}


\begin{figure}
\centering
	\includegraphics[width=.48\textwidth]{figure2.png}
	\caption{The dependency tree of S3, which partially matches a \CVT entry of \emph{people.marriage} from \FB. \label{fig:2}}
\end{figure}


Although time-related arguments are often missing in the currently imperfect \KBs, they are crucial to identify the actual occurrence of an
event. As an example, suppose we want to use the \CVT entry shown in Table~\ref{tbl:bs} to determine if sentences S2 and S3 are a
\texttt{business.acquistion} event. While this strategy works for S2, it gives a false prediction for S3. If we add the time-related
argument (i.e., date in this case), we can then correctly label both sentences. We have conducted some inital experiments on our training
data during the design phase, and found time-related arguments to be useful. Therefore, we always consider time-related arguments as key
arguments (step P2) they are available.

Finally, step P3 is based on our intuitions that two arguments involved in the same event mention are likely to be closer within the
syntactic structure. As an example, consider S4. The dependency parse tree of this sentence is given in Figure~\ref{fig:2}. As can be seen
from the diagram, although both \emph{Prince Philip} and \emph{marriage} can be matched as key arguments in a  \texttt{people.marriage}
entry, but with a distance of 3 (i.e., far from each other under our criterion) on the dependency parse tree, thus S4 will be labeled as
negative.

\subsection{Data Generation}
To generate training data, we follow a number of steps.


Our approach takes in existing structured tables or lists that are organized in a way similar to the \FB \CVT tables. The structured tables
can be obtained from an existing knowledge base, created by experts, or generated through a combination of both approaches. For each event
type, we determine the key arguments for each entry within that type used the key argument selection strategy described above. This step
produces a set of rules to be used for data labeling, where each rule contains the event type, key arguments and non-key-arguments given by
a structured table entry. We also use alias information (such as Wikipedia redirect) to match two arguments that have different literal
names but refer to the same entity (e.g. Microsoft and MS).

Next, we label each individual sentence from the target dataset. The labeling process is straightforward. We enumerate all rules from the
generate rule set, and check if the target sentence contains all the key arguments specified by a rule. We regard a sentence as a
\emph{positive} sample if it contains all the key arguments of a rule, or \emph{negative} otherwise. For insances, S1 in Figure~\ref
{fig:example} (a) and S2 (with its arguments in italics and underlined) are positive examples, while S3 and S4 are negative.

Because 68\%  of  arguments in our dataset consist of more than one word, we formulate each step in a sequence labeling paradigm rather
than word-level classifications. We tag each word of the sentence using the standard begin-inside-outside (\texttt{BIO})
scheme\FIXME{\cite{}}, where each token is labeled as \texttt{B-role} if it is the beginning of an event argument with its corresponding
role \texttt{role}, or \texttt{I-role} if it is inside an argument, or \texttt{O} otherwise. We call this a \textbf{labeling sequence}. For
instance, Figure~\ref{fig:ls} illustrates the labeling sequence for sentence S1 given in Figure~\ref{fig:example} (a).  As an output, we
record the raw text of each training sentence, plus its labeling sequence, and key/non-key arguments.

\begin{figure}[t!]
\centering
\scriptsize
\begin{tabular}{ccccccc}
\toprule
Remedy & Corp & was & sold & to & BMC & \\
\rowcolor{Gray} \texttt{B-com.\_ac.} & \texttt{I-comp.\_ac.} & \texttt{O} & \texttt{O} & \texttt{O} & \texttt{B-ac.\_comp.} &\\
\end{tabular}
\begin{tabular}{ccccc}
Software & as & the & Service &Management \\
\rowcolor{Gray} \texttt{I-acq.\_comp.} & \texttt{O} & \texttt{O} & \texttt{B-div.\_fo.} & \texttt{I-div.\_fo.} \\
Business & Unit & in & 2004 &.\\
\rowcolor{Gray} \texttt{I-div.\_fo.} & \texttt{I-div.\_fo.} & \texttt{O} & \texttt{B-date} &\\
\bottomrule
\end{tabular}

\caption{The labeling sequence for sentence S1 shown in Figure~\ref{fig:example} (a). Each token of the sentence is tagged using the \BIO
scheme. \label{fig:ls}}
\end{figure}

\subsection{Limitations}
Our approach relies on structured tables or lists to automatically label text. The table can be obtained from an existing knowledge base or
hand-crafted by developers. We stress that providing a table incurs much less overhead than manually tagging each training sentence, as a
single table entry can be automatically applied to an unbounded number of sentences.

Like all current event extractors, our approach does not support detecting event types that are not seen in the training data. However, our
approach does offer an easy way to extend the structured table to support new types, i.e., by adding new entries to the table.


Our current implementation does not disambiguate pronouns. There are co-reference methods\FIXME{~\cite{}} that can be used to find out
which word a pronoun refers to. These works are therefore complementary to our approach. Integrating our work with co-reference methods is
our future work. Finally, while this work targets sentence-level event detection, we believe it is general applicable. There are methods
like \FIXME{\cite{}} can be used to extend our approach to document-level event extraction.
